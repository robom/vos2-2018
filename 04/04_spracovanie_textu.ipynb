{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spracovanie textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *I made her duck.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uvaril (upiekol) som jej kačku.\n",
    "* Uvaril som kačku, ktorá jej patrí.\n",
    "* Spravil som jej kačku (napr. ako origami).\n",
    "* Primäl som ju zohnúť sa.\n",
    "* Premenil som ju na kačku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ako sa v tom vyznať?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lematizácia (stemming)*\n",
    "* made -> make\n",
    "\n",
    "*Word sense disambiguation*\n",
    "* make ?= uvariť, spraviť, prinútiť, ...\n",
    "\n",
    "*Part-of-speech tagging*\n",
    "* duck ako sloveso alebo podstatné meno?\n",
    "\n",
    "*Coreference (anaphora) resolution*\n",
    "* her -> ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ďalšie NLP úlohy\n",
    "\n",
    "* Segmentácia textu\n",
    "  * na tokeny\n",
    "  * na vety (sentence boundary disambiguation)\n",
    "  * na témy\n",
    "* Automatická sumarizácia textu\n",
    "* Analýza sentimentu\n",
    "* Strojový preklad textu\n",
    "* (Syntaktické) parsovanie viet\n",
    "* Analýza diskurzu, odpovedanie na otázky \n",
    "* Rozpoznávanie a segmentácia reči\n",
    "  * Speech-to-text, text-to-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nás zaujíma predovšetkým extrakcia čŕt z textu\n",
    "\n",
    "Segmentácia textu + identifikácia kľúčových slov, resp. často sa spolu vyskytujúcich slov (tokenov)\n",
    "\n",
    "Určovanie podobnosti dvoch textových dokumentov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostupné nástroje v Pythone\n",
    "\n",
    "NLTK (http://www.nltk.org/)\n",
    "\n",
    "Gensim (https://radimrehurek.com/gensim/tutorial.html)\n",
    "\n",
    "### Ďalšie nástroje (mimo Pythonu)\n",
    "\n",
    "Stanford CoreNLP (https://stanfordnlp.github.io/CoreNLP/; rozhranie aj cez NLTK)\n",
    "\n",
    "Apache OpenNLP (https://opennlp.apache.org/)\n",
    "\n",
    "WordNet (https://wordnet.princeton.edu/; rozhranie cez NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"At eight o'clock on Thursday morning \n",
    "... Arthur didn't feel very good. He closed his eyes and went to bed again.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"At eight o'clock on Thursday morning \\nArthur didn't feel very good.\",\n",
       " 'He closed his eyes and went to bed again.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['At',\n",
       "  'eight',\n",
       "  \"o'clock\",\n",
       "  'on',\n",
       "  'Thursday',\n",
       "  'morning',\n",
       "  'Arthur',\n",
       "  'did',\n",
       "  \"n't\",\n",
       "  'feel',\n",
       "  'very',\n",
       "  'good',\n",
       "  '.'],\n",
       " ['He', 'closed', 'his', 'eyes', 'and', 'went', 'to', 'bed', 'again', '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['At',\n",
       "  'eight',\n",
       "  \"o'clock\",\n",
       "  'on',\n",
       "  'thursday',\n",
       "  'morn',\n",
       "  'arthur',\n",
       "  'did',\n",
       "  \"n't\",\n",
       "  'feel',\n",
       "  'veri',\n",
       "  'good',\n",
       "  '.'],\n",
       " ['He', 'close', 'hi', 'eye', 'and', 'went', 'to', 'bed', 'again', '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[porter.stem(token) for token in sent] for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematizácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['At',\n",
       "  'eight',\n",
       "  \"o'clock\",\n",
       "  'on',\n",
       "  'Thursday',\n",
       "  'morning',\n",
       "  'Arthur',\n",
       "  'did',\n",
       "  \"n't\",\n",
       "  'feel',\n",
       "  'very',\n",
       "  'good',\n",
       "  '.'],\n",
       " ['He', 'closed', 'his', 'eye', 'and', 'went', 'to', 'bed', 'again', '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[wnl.lemmatize(token) for token in sent] for sent in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('At', 'IN'),\n",
       "  ('eight', 'CD'),\n",
       "  (\"o'clock\", 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('Thursday', 'NNP'),\n",
       "  ('morning', 'NN'),\n",
       "  ('Arthur', 'NNP'),\n",
       "  ('did', 'VBD'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('feel', 'VB'),\n",
       "  ('very', 'RB'),\n",
       "  ('good', 'JJ'),\n",
       "  ('.', '.')],\n",
       " [('He', 'PRP'),\n",
       "  ('closed', 'VBD'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('eyes', 'NNS'),\n",
       "  ('and', 'CC'),\n",
       "  ('went', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('bed', 'VB'),\n",
       "  ('again', 'RB'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = [nltk.pos_tag(sent) for sent in tokens]\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('IN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menné entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n"
     ]
    }
   ],
   "source": [
    "print(entities.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = nltk.word_tokenize(\"He is very old. He is very bold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'is'),\n",
       " ('is', 'very'),\n",
       " ('very', 'old'),\n",
       " ('old', '.'),\n",
       " ('.', 'He'),\n",
       " ('He', 'is'),\n",
       " ('is', 'very'),\n",
       " ('very', 'bold'),\n",
       " ('bold', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(text2))\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('.', 'He'): 1,\n",
       "          ('He', 'is'): 2,\n",
       "          ('bold', '.'): 1,\n",
       "          ('is', 'very'): 2,\n",
       "          ('old', '.'): 1,\n",
       "          ('very', 'bold'): 1,\n",
       "          ('very', 'old'): 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'is', 'very'),\n",
       " ('is', 'very', 'old'),\n",
       " ('very', 'old', '.'),\n",
       " ('old', '.', 'He'),\n",
       " ('.', 'He', 'is'),\n",
       " ('He', 'is', 'very'),\n",
       " ('is', 'very', 'bold'),\n",
       " ('very', 'bold', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lexikálna databáza\n",
    "* Obsahuje synsety\n",
    "  * Podstatné mená, slovesá, prídavné mená, príslovky\n",
    "* Prepojenia medzi synsetmi\n",
    "  * Antonymá, hyperonymá, hyponymá, holonymá, meronymá\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('accelerator.n.01'),\n",
       " Synset('air_bag.n.01'),\n",
       " Synset('auto_accessory.n.01'),\n",
       " Synset('automobile_engine.n.01'),\n",
       " Synset('automobile_horn.n.01'),\n",
       " Synset('buffer.n.06'),\n",
       " Synset('bumper.n.02'),\n",
       " Synset('car_door.n.01'),\n",
       " Synset('car_mirror.n.01'),\n",
       " Synset('car_seat.n.01'),\n",
       " Synset('car_window.n.01'),\n",
       " Synset('fender.n.01'),\n",
       " Synset('first_gear.n.01'),\n",
       " Synset('floorboard.n.02'),\n",
       " Synset('gasoline_engine.n.01'),\n",
       " Synset('glove_compartment.n.01'),\n",
       " Synset('grille.n.02'),\n",
       " Synset('high_gear.n.01'),\n",
       " Synset('hood.n.09'),\n",
       " Synset('luggage_compartment.n.01'),\n",
       " Synset('rear_window.n.01'),\n",
       " Synset('reverse.n.02'),\n",
       " Synset('roof.n.02'),\n",
       " Synset('running_board.n.01'),\n",
       " Synset('stabilizer_bar.n.01'),\n",
       " Synset('sunroof.n.01'),\n",
       " Synset('tail_fin.n.02'),\n",
       " Synset('third_gear.n.01'),\n",
       " Synset('window.n.02')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('white.n.02.white')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('black')[0].lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprezentácia textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textový dokument väčšinou reprezentujeme pomocou množiny slov (angl. *bag-of-words*) = vektorom. Zložky vektoru predstavujú jednotlivé slová, resp. n-gramy zo slovníka (pre celý korpus/jazyk). Hodnotou zložiek vektora môže byť:\n",
    "\n",
    "* početnosť\n",
    "* frekvencia\n",
    "* váhovaná frekvencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slová s vysokou frekvenciou výskytu v jazyku (spojky a pod.) sa označujú ako tzv. *stop slová* a zvyknú sa pri predspracovaní odstraňovať."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency * inverse document frequency\n",
    "\n",
    "`TF` – frekvencia slova v aktuálnom dokumente\n",
    "\n",
    "`IDF` – záporný logaritmus pravdepodobnosti výskytu slova v dokumente (rovnaká pre všetky dokumenty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rôzne varianty (váhovacie schémy): https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TF](img/tf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IDF](img/idf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TF-IDF](img/tfidf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knižnica na modelovanie tém v dokumentoch.\n",
    "\n",
    "Implementuje TF-IDF, LSA, pLSA, LDA, HDP, DTM, word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robom\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human machine interface for lab abc computer applications',\n",
       " 'A survey of user opinion of computer system response time',\n",
       " 'The EPS user interface management system',\n",
       " 'System and human system engineering testing of EPS',\n",
       " 'Relation of user perceived response time to error measurement',\n",
       " 'The generation of random binary unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV Widths of trees and well quasi ordering',\n",
       " 'Graph minors A survey']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odstránenie stopslov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odstránenie slov, ktoré sa v korpuse vyskytujú len raz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"Human computer interaction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénovanie TF-IDF modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.7071067811865476), (1, 0.7071067811865476)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[new_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ďalšie modely: LSI, LDA, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Každé slovo má naučený vektor reálnych čísel, ktoré reprezentujú rôzne jeho vlastnosti a zachytávajú viaceré lingvistické pravidelnosti. Môžeme počítať podobnosť medzi slovami ako podobnosť dvoch vektorov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector('Paris') - vector('France') + vector('Italy') ~= vector('Rome')\n",
    "\n",
    "vector('king') - vector('man') + vector('woman') ~= vector('queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
